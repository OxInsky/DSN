# DSN分割肝脏实验总结 #
## 代码文件备注 ##
+ DSN.py：DSN网络架构的代码；
+ data_preprocess.py：将标签数据转化成 `0 1`；
+ Data_Generator.py：读取数据，生成yeild生成器，传入网络结构。
## 前言 ##
+ 本实验采用3D深度监督网络（DSN）对肝脏进行分割，因为使用3D的肝脏数据进行分割可以很好的体积上下文信息。
+ DSN的大致学习过程是：基于CNN，为了应对梯度消失和模型辨别能力问题，在隐藏层加入一些额外的监督来抵消梯度消失的不利影响。具体而言，使用一些额外的反卷积层来扩展一些低级和中级特征向量。然后使用softmax层来获得用于计算分类误差的dense预测(监督层的预测)。**利用从这些分支预测和最后输出层得到的梯度，可以有效地减轻梯度消失的影响。
+ deeply- supervision 的优点包括：
	1. 能够减轻梯度爆炸或梯度消失，收敛速度更快（辅助 loss 将误差直接注入中间层，有点类似于 resnet 的机制，不同的是 loss 来源不同 ）;
	2. 辅助 loss 起到 regularization 的作用.


+ 详情可参见这篇文章《[论文解读：Deeply-Supervised Nets](http://tanqingbo.com/2018/11/12/Deeply-Supervised%20Nets/)》和《[论文笔记：3D Deeply Supervised Network for Automatic Liver Segmentation from CT Volumes](http://tanqingbo.com/2018/11/13/3D%20Deeply%20Supervised%20Network%20for%20Automatic%20Liver%20Segmentation%20from%20CT%20Volumes/)》

## 数据问题 ##
### `one_hot`处理 ###
+ 在开始训练DSN网络之前要对数据进行一些预处理，首先将标签数据转成`0 1`值，即前景的像素值为1，背景的像素值为0。因为再算loss的时候，要将金标准标签转成`one_hot`值，想把标签处理成`0 1`方便做**`one_hot`处理。**
+ 简单解释一下什么是`one_hot`，one-hot code也称独热码，通常用于分类任务中作为最后的FC层的输出。在机器学习中对于离散型的分类型的数据，需要对其进行数字化，比如说对性别这一属性，只有男女两种值，用数字化表达，指定男性为0，女性为1，那么一个特征向量(1,0,1),转换成独热码（`one_hot`）就变成([1,0],[0,1],[1,0])。

### 内存不足问题 ###
+ 因为3D肝脏数据，数据量庞大，如果一次性把整组的数据都加载进去可能导致计算内存不足的问题，所以在训练之前需要减少训练数据的`size`，本实验中将所有的数据都转换成了`[16,512,512]`大小。
+ 还有也是因为内存不足的问题，再加载数据的时候用到了`yield`迭代器，需要注意的是，它`yield`是一个类似 `return` 的关键字，迭代一次遇到`yield`时就返回`yield`后面的值。重点是：下一次迭代时，从上一次迭代遇到的yield后面的代码开始执行。详细理解`yield`可以参考文章《[彻底理解python中的yeild](http://tanqingbo.com/2018/11/27/%E5%BD%BB%E5%BA%95%E7%90%86%E8%A7%A3python%E4%B8%AD%E7%9A%84yeild/)》



## 代码解读 ##
+ 网络的架构大致如下：

			layers = [['block0', [['conv', [9, 9, 7, 1, 8], [1, 1, 1, 1, 1], 'SAME', 0.7]]],
			          ['block1', [['conv', [9, 9, 7, 8, 16], [1, 1, 1, 1, 1], 'SAME', 0.7],
			                      ['maxpool', [1, 2, 2, 2, 1], [1, 2, 2, 2, 1], 'SAME']]],
			          ['block2', [['conv', [7, 7, 5, 16, 32], [1, 1, 1, 1, 1], 'SAME', 0.7]]],
			          ['block3', [['conv', [7, 7, 5, 32, 32], [1, 1, 1, 1, 1], 'SAME', 0.7],
			                      ['maxpool', [1, 2, 2, 2, 1], [1, 2, 2, 2, 1], 'SAME']], ],
			          ['block4', [['conv', [5, 5, 3, 32, 32], [1, 1, 1, 1, 1], 'SAME', 0.7]]],
			          ['block5', [['conv', [1, 1, 1, 32, 32], [1, 1, 1, 1, 1], 'SAME', 0.7]]],
			          ['block6', [['deconv', [3, 3, 3, 32, 32], [1, 2, 2, 2, 1], 'SAME']]],
			          ['block7', [['deconv', [3, 3, 3, 2, 32], [1, 2, 2, 2, 1], 'SAME']]],
			          ]
+ 在`['block0', [['conv', [9, 9, 7, 1, 8], [1, 1, 1, 1, 1], 'SAME', 0.7]]]`中，`[9, 9, 7, 1, 8]`表示卷积核的大小为[9,9,7],当前深度为1，卷积核的深度为7.`SAME`表示在卷积之前加了`padding`是特征图在卷积前后特征不变。0.7是`dropout`系数。